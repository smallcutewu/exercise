{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "\n",
    "start_token = 'G'\n",
    "end_token = 'E'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_poems(file_name):\n",
    "    poems = []\n",
    "    with open(file_name, \"r\", encoding='utf-8', ) as f:\n",
    "        for line in f.readlines():\n",
    "            try:\n",
    "                title, content = line.strip().split(':')\n",
    "                content = content.replace(' ', '')\n",
    "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content or \\\n",
    "                                start_token in content or end_token in content:\n",
    "                    continue\n",
    "                if len(content) < 5 or len(content) > 80:\n",
    "                    continue\n",
    "                content = start_token + content + end_token\n",
    "                poems.append(content)\n",
    "            except ValueError as e:\n",
    "                pass\n",
    "    # 按诗的字数排序\n",
    "    poems = sorted(poems, key=lambda line: len(line))\n",
    "    # 统计每个字出现次数\n",
    "    all_words = []\n",
    "    for poem in poems:\n",
    "        all_words += [word for word in poem]  \n",
    "    counter = collections.Counter(all_words)  # 统计词和词频。\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*count_pairs)\n",
    "    words = words[:len(words)] + (' ',)\n",
    "    word_int_map = dict(zip(words, range(len(words))))\n",
    "    poems_vector = [list(map(word_int_map.get, poem)) for poem in poems]\n",
    "    return poems_vector, word_int_map, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rnn_lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(model, input_data, output_data, vocab_size, rnn_size=128, num_layers=2, batch_size=64,\n",
    "              learning_rate=0.01):\n",
    "    end_points = {}\n",
    "    # 构建RNN基本单元RNNcell\n",
    "    if model == 'rnn':\n",
    "        cell_fun = tf.contrib.rnn.BasicRNNCell\n",
    "    elif model == 'gru':\n",
    "        cell_fun = tf.contrib.rnn.GRUCell\n",
    "    else:\n",
    "        cell_fun = tf.contrib.rnn.BasicLSTMCell\n",
    "    #？？？？？？？？？？？？？？？？？？？？？？\n",
    "    # 每层128个小单元，一共有两层，输出的Ct 和 Ht 要分开放到两个tuple中\n",
    "    # 在下面补全代码 \n",
    "    #################################################\n",
    "    cell = cell_fun( rnn_size, state_is_tuple=True )\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(  [cell] * num_layers, state_is_tuple=True )\n",
    "    #################################################\n",
    "    # 如果是训练模式，output_data不为None，则初始状态shape为[batch_size * rnn_size]\n",
    "    # 如果是生成模式，output_data为None，则初始状态shape为[1 * rnn_size]\n",
    "    if output_data is not None:\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    else:\n",
    "        initial_state = cell.zero_state(1, tf.float32)\n",
    "\n",
    "    # 构建隐层\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.Variable(tf.random_uniform([vocab_size + 1, rnn_size], -1.0, 1.0),name = 'embedding')\n",
    "        inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    #？？？？？？？？？？？？？？？？？？？？？？？？？？\n",
    "    ####################################################    \n",
    "    outputs, last_state = tf.nn.dynamic_rnn(  cell, inputs, initial_state=initial_state )# 填写里面的内容\n",
    "    ######################################################\n",
    "    output = tf.reshape(outputs, [-1, rnn_size])\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([rnn_size, vocab_size + 1]))\n",
    "    bias = tf.Variable(tf.zeros(shape=[vocab_size + 1]))\n",
    "    logits = tf.nn.bias_add(tf.matmul(output, weights), bias=bias) # 一层全连接\n",
    "\n",
    "\n",
    "    if output_data is not None: # 训练模式\n",
    "        labels = tf.one_hot(tf.reshape(output_data, [-1]), depth=vocab_size + 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)  # 优化器用的 adam\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['output'] = output\n",
    "        end_points['train_op'] = train_op\n",
    "        end_points['total_loss'] = total_loss\n",
    "        end_points['loss'] = loss\n",
    "        end_points['last_state'] = last_state\n",
    "        #自己test cell\n",
    "        end_points['cell']=cell\n",
    "    else: # 生成模式\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['last_state'] = last_state\n",
    "        end_points['prediction'] = prediction\n",
    "        end_points['cell']=cell\n",
    "    return end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    # 处理数据集\n",
    "    poems_vector, word_to_int, vocabularies = process_poems('../poems.txt')\n",
    "    # 生成batch\n",
    "    batches_inputs, batches_outputs = generate_batch(64, poems_vector, word_to_int)\n",
    "\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    output_targets = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    # 构建模型\n",
    "    end_points = rnn_model(model='lstm', input_data=input_data, output_data=output_targets, vocab_size=len(\n",
    "        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=0.01)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        for epoch in range(2):\n",
    "            n = 0\n",
    "            n_chunk = len(poems_vector) // batch_size\n",
    "            for batch in range(n_chunk):\n",
    "                loss, _, _ = sess.run([\n",
    "                    end_points['total_loss'],\n",
    "                    end_points['last_state'],\n",
    "                    end_points['train_op']\n",
    "                ], feed_dict={input_data: batches_inputs[n], output_targets: batches_outputs[n]})\n",
    "                n += 1\n",
    "                print('[INFO] Epoch: %d , batch: %d , training loss: %.6f' % (epoch, batch, loss))\n",
    "        saver.save(sess, './poem_generator')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成 诗歌部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_poem(begin_word):\n",
    "    batch_size = 1\n",
    "    poems_vector, word_int_map, vocabularies = process_poems('../poems.txt')\n",
    "\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "\n",
    "    end_points = rnn_model(model='lstm', input_data=input_data, output_data=None, vocab_size=len(\n",
    "        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=0.01)\n",
    "    # 如果指定开始的字\n",
    "    if begin_word:\n",
    "        word = begin_word\n",
    "    else:\n",
    "        word = to_word(predict, vocabularies)\n",
    "        \n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "#     _, last_state, probs, cell, initial_state = neural_network()\n",
    "    probs=end_points['prediction']\n",
    "    last_state=end_points['last_state']\n",
    "    cell=end_points['cell']\n",
    "    initial_state=end_points['initial_state']\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        saver.restore(sess, './poem_generator')# 恢复之前训练好的模型 \n",
    "        poem = ''\n",
    "        #???????????????????????????????????????\n",
    "        # 下面部分代码主要功能是根据指定的开始字符来生成诗歌\n",
    "        #########################################\n",
    "        state_ = sess.run(cell.zero_state(1, tf.float32))\n",
    "        x = np.array([list(map(word_int_map.get, '['))])\n",
    "        [probs_, state_] = sess.run([probs, last_state], feed_dict={input_data: x, initial_state: state_})\n",
    "        word = to_word(probs_,vocabularies)\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        for word in head:\n",
    "            while word != '，' and word != '。':\n",
    "                poem += word\n",
    "                x = np.array([list(map(word_int_map.get, word))])\n",
    "                [probs_, state_] = sess.run([probs, last_state], feed_dict={input_data: x, initial_state: state_})\n",
    "                word = to_word(probs_,vocabularies)\n",
    "                time.sleep(1)\n",
    "            if i % 2 == 0:\n",
    "                poem += '，'\n",
    "            else:\n",
    "                poem += '。'\n",
    "            i += 1\n",
    "        \n",
    "\n",
    "        \n",
    "        #########################################\n",
    "        return poem\n",
    "    \n",
    "####################\n",
    "\n",
    "def gen_poetry():\n",
    "    \n",
    "    batch_size = 1\n",
    "    poems_vector, word_int_map, vocabularies = process_poems('../poems.txt')\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    end_points = rnn_model(model='lstm', input_data=input_data, output_data=None, vocab_size=len(\n",
    "        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=0.01)\n",
    "#     _, last_state, probs, cell, initial_state = neural_network()\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "#     _, last_state, probs, cell, initial_state = neural_network()\n",
    "    probs=end_points['prediction']\n",
    "    last_state=end_points['last_state']\n",
    "    cell=end_points['cell']\n",
    "    initial_state=end_points['initial_state']\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    " \n",
    "        saver.restore(sess, 'poem_generator')\n",
    " \n",
    "        state_ = sess.run(cell.zero_state(1, tf.float32))\n",
    " \n",
    "        x = np.array([list(map(word_int_map.get, '['))])\n",
    "        [probs_, state_] = sess.run([probs, last_state], feed_dict={input_data: x, initial_state: state_})\n",
    "        word = to_word(probs_,vocabularies)\n",
    "        #word = words[np.argmax(probs_)]\n",
    "        poem = ''\n",
    "        while word != ']':\n",
    "            poem += word\n",
    "            x = np.zeros((1,1))\n",
    "            x[0,0] = word_int_map[word]\n",
    "            [probs_, state_] = sess.run([probs, last_state], feed_dict={input_data: x, initial_state: state_})\n",
    "            word = to_word(probs_,vocabularies)\n",
    "            #word = words[np.argmax(probs_)]\n",
    "        return poem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他的一些处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, poems_vec, word_to_int):\n",
    "    # 每次取64首诗进行训练\n",
    "    n_chunk = len(poems_vec) // batch_size\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "\n",
    "        batches = poems_vec[start_index:end_index]\n",
    "        # 找到这个batch的所有poem中最长的poem的长度\n",
    "        length = max(map(len, batches))\n",
    "        # 填充一个这么大小的空batch，空的地方放空格对应的index标号\n",
    "        x_data = np.full((batch_size, length), word_to_int[' '], np.int32)\n",
    "        for row in range(batch_size):\n",
    "            x_data[row, :len(batches[row])] = batches[row]\n",
    "        y_data = np.copy(x_data)\n",
    "        y_data[:, :-1] = x_data[:, 1:]\n",
    "        \"\"\"\n",
    "        x_data             y_data\n",
    "        [6,2,4,6,9]       [2,4,6,9,9]\n",
    "        [1,4,2,8,5]       [4,2,8,5,5]\n",
    "        \"\"\"\n",
    "        x_batches.append(x_data)\n",
    "        y_batches.append(y_data)\n",
    "    return x_batches, y_batches\n",
    "\n",
    "def to_word(predict, vocabs):# 预测的结果转化成汉字\n",
    "    sample = np.argmax(predict)\n",
    "    if sample > len(vocabs):\n",
    "        sample = len(vocabs) - 1\n",
    "    return vocabs[sample]\n",
    "def pretty_print_poem(poem):#  令打印的结果更工整\n",
    "    poem_sentences = poem.split('。')\n",
    "    for s in poem_sentences:\n",
    "        if s != '' and len(s) > 10:\n",
    "            print(s + '。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train tang poem...\n",
      "[INFO] write tang poem...\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "INFO:tensorflow:Restoring parameters from poem_generator\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cf73fa6ce008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#run_training() # 训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO] write tang poem...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpoem2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_poetry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# 生成诗歌\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpretty_print_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoem2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-22b5178c2431>\u001b[0m in \u001b[0;36mgen_poetry\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_int_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'['\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mprobs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabularies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m#word = words[np.argmax(probs_)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "print('[INFO] train tang poem...')\n",
    "#run_training() # 训练模型\n",
    "print('[INFO] write tang poem...')\n",
    "poem2 = gen_poetry()# 生成诗歌\n",
    "print(\"#\" * 25)\n",
    "pretty_print_poem(poem2)\n",
    "print('#' * 25)\n",
    "#训练模型时间比较长，训练模型完成后每次生成诗歌的时，不需要再次训练 ，可以注销上面的 run_training()。生成部分执行速度很快"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python-tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
